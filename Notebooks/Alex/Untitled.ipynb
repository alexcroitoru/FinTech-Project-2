{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc10523-87fc-409f-9ff6-e60c7bfef1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "name 'blacklist' is not defined\n",
      "It took 25.82 seconds to analyze 191 comments in 17 posts in 1 subreddits.\n",
      "\n",
      "Posts analyzed saved in titles\n",
      "\n",
      "10 most mentioned tickers: \n",
      "\n",
      "Sentiment analysis of top 5 picks:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 0 elements, new values have 4 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-340ada277515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-340ada277515>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpicks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_analyzed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicks_ayz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_comments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[0mvisualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicks_ayz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpicks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-340ada277515>\u001b[0m in \u001b[0;36mvisualization\u001b[1;34m(picks_ayz, scores, picks, times, top)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nSentiment analysis of top {picks_ayz} picks:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Bearish'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Neutral'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Bullish'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Total/Compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5476\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5477\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5479\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5480\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    221\u001b[0m                 \u001b[1;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;34mf\"values have {new_len} elements\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 4 elements"
     ]
    }
   ],
   "source": [
    "'''*****************************************************************************\n",
    "Purpose: To analyze the sentiments of the reddit\n",
    "This program uses Vader SentimentIntensityAnalyzer to calculate the ticker compound value. \n",
    "You can change multiple parameters to suit your needs. See below under \"set program parameters.\"\n",
    "Implementation:\n",
    "I am using sets for 'x in s' comparison, sets time complexity for \"x in s\" is O(1) compare to list: O(n).\n",
    "Limitations:\n",
    "It depends mainly on the defined parameters for current implementation:\n",
    "It completely ignores the heavily downvoted comments, and there can be a time when\n",
    "the most mentioned ticker is heavily downvoted, but you can change that in upvotes variable.\n",
    "Author: github:asad70\n",
    "-------------------------------------------------------------------\n",
    "****************************************************************************'''\n",
    "\n",
    "import praw\n",
    "from data import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji    # removes emojis\n",
    "import re   # removes links\n",
    "import en_core_web_sm\n",
    "import string\n",
    "\n",
    "\n",
    "def data_extractor(reddit):\n",
    "    '''extracts all the data from reddit\n",
    "    Parameter: reddt: reddit obj\n",
    "    Return:    posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz\n",
    "                \n",
    "                posts: int: # of posts analyzed\n",
    "                 c_analyzed: int: # of comments analyzed\n",
    "                 tickers: dict: all the tickers found\n",
    "                titles: list: list of the title of posts analyzed \n",
    "                 a_comments: dict: all the comments to analyze\n",
    "                 picks: int: top picks to analyze\n",
    "                 subs: int: # of subreddits analyzed\n",
    "                picks_ayz: int: top picks to analyze\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''############################################################################'''\n",
    "    # set the program parameters\n",
    "    subs = ['wallstreetbets' ]     # sub-reddit to search\n",
    "    post_flairs = {'Daily Discussion', 'Weekend Discussion', 'Discussion'}    # posts flairs to search || None flair is automatically considered\n",
    "    goodAuth = {'AutoModerator'}   # authors whom comments are allowed more than once\n",
    "    uniqueCmt = True                # allow one comment per author per symbol\n",
    "    ignoreAuthP = {'example'}       # authors to ignore for posts \n",
    "    ignoreAuthC = {'example'}       # authors to ignore for comment \n",
    "    upvoteRatio = 0.70         # upvote ratio for post to be considered, 0.70 = 70%\n",
    "    ups = 20       # define # of upvotes, post is considered if upvotes exceed this #\n",
    "    limit = 1     # define the limit, comments 'replace more' limit\n",
    "    upvotes = 2     # define # of upvotes, comment is considered if upvotes exceed this #\n",
    "    picks = 10     # define # of picks here, prints as \"Top ## picks are:\"\n",
    "    picks_ayz = 5   # define # of picks for sentiment analysis\n",
    "    '''############################################################################'''     \n",
    "    \n",
    "    posts, count, c_analyzed, tickers, titles, a_comments = 0, 0, 0, {}, [], {}\n",
    "    cmt_auth = {}\n",
    "    \n",
    "    for sub in subs:\n",
    "        subreddit = reddit.subreddit(sub)\n",
    "        hot_python = subreddit.hot()    # sorting posts by hot\n",
    "        # Extracting comments, symbols from subreddit\n",
    "        for submission in hot_python:\n",
    "            flair = submission.link_flair_text \n",
    "            author = submission.author.name         \n",
    "            \n",
    "            # checking: post upvote ratio # of upvotes, post flair, and author \n",
    "            if submission.upvote_ratio >= upvoteRatio and submission.ups > ups and (flair in post_flairs or flair is None) and author not in ignoreAuthP:   \n",
    "                submission.comment_sort = 'new'     \n",
    "                comments = submission.comments\n",
    "                titles.append(submission.title)\n",
    "                posts += 1\n",
    "                try: \n",
    "                    submission.comments.replace_more(limit=limit)   \n",
    "                    for comment in comments:\n",
    "                        # try except for deleted account?\n",
    "                        try: auth = comment.author.name\n",
    "                        except: pass\n",
    "                        c_analyzed += 1\n",
    "                        \n",
    "                        # checking: comment upvotes and author\n",
    "                        if comment.score > upvotes and auth not in ignoreAuthC:      \n",
    "                            split = comment.body.split(\" \")\n",
    "                            for word in split:\n",
    "                                word = word.replace(\"$\", \"\")        \n",
    "                                # upper = ticker, length of ticker <= 5, excluded words,                     \n",
    "                                if word.isupper() and len(word) <= 5 and word not in blacklist and word in us:\n",
    "                                    \n",
    "                                    # unique comments, try/except for key errors\n",
    "                                    if uniqueCmt and auth not in goodAuth:\n",
    "                                        try: \n",
    "                                            if auth in cmt_auth[word]: break\n",
    "                                        except: pass\n",
    "                                        \n",
    "                                    # counting tickers\n",
    "                                    if word in tickers:\n",
    "                                        tickers[word] += 1\n",
    "                                        a_comments[word].append(comment.body)\n",
    "                                        cmt_auth[word].append(auth)\n",
    "                                        count += 1\n",
    "                                    else:                               \n",
    "                                        tickers[word] = 1\n",
    "                                        cmt_auth[word] = [auth]\n",
    "                                        a_comments[word] = [comment.body]\n",
    "                                        count += 1   \n",
    "                except Exception as e: print(e)\n",
    "                \n",
    "                           \n",
    "    return posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz\n",
    "\n",
    "\n",
    "def print_helper(tickers, picks, c_analyzed, posts, subs, titles, time, start_time):\n",
    "    '''prints out top tickers, and most mentioned tickers\n",
    "    \n",
    "    Parameter:   tickers: dict: all the tickers found\n",
    "                 picks: int: top picks to analyze\n",
    "                 c_analyzed: int: # of comments analyzed\n",
    "                 posts: int: # of posts analyzed\n",
    "                 subs: int: # of subreddits analyzed\n",
    "                titles: list: list of the title of posts analyzed \n",
    "                 time: time obj: top picks to analyze\n",
    "                start_time: time obj: prog start time\n",
    "    Return: symbols: dict: dict of sorted tickers based on mentions\n",
    "            times: list: include # of time top tickers is mentioned\n",
    "            top: list: list of top tickers\n",
    "    '''    \n",
    "\n",
    "    # sorts the dictionary\n",
    "    symbols = dict(sorted(tickers.items(), key=lambda item: item[1], reverse = True))\n",
    "    top_picks = list(symbols.keys())[0:picks]\n",
    "    time = (time.time() - start_time)\n",
    "    \n",
    "    # print top picks\n",
    "    print(\"It took {t:.2f} seconds to analyze {c} comments in {p} posts in {s} subreddits.\\n\".format(t=time, c=c_analyzed, p=posts, s=len(subs)))\n",
    "    print(\"Posts analyzed saved in titles\")\n",
    "    #for i in titles: print(i)  # prints the title of the posts analyzed\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{picks} most mentioned tickers: \")\n",
    "    times = []\n",
    "    top = []\n",
    "    for i in top_picks:\n",
    "        print(f\"{i}: {symbols[i]}\")\n",
    "        times.append(symbols[i])\n",
    "        top.append(f\"{i}: {symbols[i]}\")\n",
    "   \n",
    "    return symbols, times, top\n",
    "    \n",
    "    \n",
    "def sentiment_analysis(picks_ayz, a_comments, symbols):\n",
    "    '''analyzes sentiment anaylsis of top tickers\n",
    "    \n",
    "    Parameter:   picks_ayz: int: top picks to analyze\n",
    "                 a_comments: dict: all the comments to analyze\n",
    "                 symbols: dict: dict of sorted tickers based on mentions\n",
    "    Return:      scores: dictionary: dictionary of all the sentiment analysis\n",
    "    '''\n",
    "    scores = {}\n",
    "     \n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "   # vader.lexicon.update(new_words)     # adding custom words from data.py \n",
    "    picks_sentiment = list(symbols.keys())[0:picks_ayz]\n",
    "    \n",
    "    for symbol in picks_sentiment:\n",
    "        stock_comments = a_comments[symbol]\n",
    "        for cmnt in stock_comments:\n",
    "    \n",
    "            emojiless = emoji.get_emoji_regexp().sub(u'', cmnt) # remove emojis\n",
    "            \n",
    "            # remove punctuation\n",
    "            text_punc  = \"\".join([char for char in emojiless if char not in string.punctuation])\n",
    "            text_punc = re.sub('[0-9]+', '', text_punc)\n",
    "                \n",
    "            # tokenizeing and cleaning \n",
    "            tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
    "            tokenized_string = tokenizer.tokenize(text_punc)\n",
    "            lower_tokenized = [word.lower() for word in tokenized_string] # convert to lower case\n",
    "            \n",
    "            # remove stop words\n",
    "            nlp = en_core_web_sm.load()\n",
    "            stopwords = nlp.Defaults.stop_words\n",
    "            sw_removed = [word for word in lower_tokenized if not word in stopwords]\n",
    "            \n",
    "            # normalize the words using lematization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = ([lemmatizer.lemmatize(w) for w in sw_removed])\n",
    "            \n",
    "            # calculating sentiment of every word in comments n combining them\n",
    "            score_cmnt = {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "            \n",
    "            word_count = 0\n",
    "            for word in lemmatized_tokens:\n",
    "                if word.upper() not in us:\n",
    "                    score = vader.polarity_scores(word)\n",
    "                    word_count += 1\n",
    "                    for key, _ in score.items():\n",
    "                        score_cmnt[key] += score[key]    \n",
    "                else:\n",
    "                    score_cmnt['pos'] = 2.0               \n",
    "                    \n",
    "            # calculating avg.\n",
    "            try:        # handles: ZeroDivisionError: float division by zero\n",
    "                for key in score_cmnt:\n",
    "                    score_cmnt[key] = score_cmnt[key] / word_count\n",
    "            except: pass\n",
    "                \n",
    "            \n",
    "            # adding score the the specific symbol\n",
    "            if symbol in scores:\n",
    "                for key, _ in score_cmnt.items():\n",
    "                    scores[symbol][key] += score_cmnt[key]\n",
    "            else:\n",
    "                scores[symbol] = score_cmnt        \n",
    "    \n",
    "        # calculating avg.\n",
    "        for key in score_cmnt:\n",
    "            scores[symbol][key] = scores[symbol][key] / symbols[symbol]\n",
    "            scores[symbol][key]  = \"{pol:.3f}\".format(pol=scores[symbol][key])\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "def visualization(picks_ayz, scores, picks, times, top):\n",
    "    '''prints sentiment analysis\n",
    "       makes a most mentioned picks chart\n",
    "       makes a chart of sentiment analysis of top picks\n",
    "       \n",
    "    Parameter:   picks_ayz: int: top picks to analyze\n",
    "                 scores: dictionary: dictionary of all the sentiment analysis\n",
    "                 picks: int: most mentioned picks\n",
    "                times: list: include # of time top tickers is mentioned\n",
    "                top: list: list of top tickers\n",
    "    Return:       None\n",
    "    '''\n",
    "    \n",
    "    # printing sentiment analysis \n",
    "    print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "    df = pd.DataFrame(scores)\n",
    "    df.index = ['Bearish', 'Neutral', 'Bullish', 'Total/Compound']\n",
    "    df = df.T\n",
    "    print(df)\n",
    "    \n",
    "    # Date Visualization\n",
    "    # most mentioned picks    \n",
    "    squarify.plot(sizes=times, label=top, alpha=.7 )\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{picks} most mentioned picks\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    df = df.astype(float)\n",
    "    colors = ['red', 'springgreen', 'forestgreen', 'coral']\n",
    "    df.plot(kind = 'bar', color=colors, title=f\"Sentiment analysis of top {picks_ayz} picks:\")\n",
    "    \n",
    "    \n",
    "    #plt.show()\n",
    "\n",
    "def main():\n",
    "    '''main function\n",
    "    Parameter:   None\n",
    "    Return:       None\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # reddit client\n",
    "    reddit = praw.Reddit(\n",
    "            client_id=\"ZjPaBT8ar8gCw7fwxI3d-Q\",\n",
    "            client_secret=\"HkSP6Qmf5cC5Pryh1e8JVasGiWo5Xg\",\n",
    "            password=\"1440falcon\",  # pass Session\"\n",
    "            user_agent=\"API\",\n",
    "            username=\"alexcroitoru\")\n",
    "\n",
    "    posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz = data_extractor(reddit)\n",
    "    symbols, times, top = print_helper(tickers, picks, c_analyzed, posts, subs, titles, time, start_time)\n",
    "    scores = sentiment_analysis(picks_ayz, a_comments, symbols)\n",
    "    visualization(picks_ayz, scores, picks, times, top)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b47c35-7c36-4856-b892-8fb6187f265a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
